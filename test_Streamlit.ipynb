{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO9XCkmbwSQ+j0i4+TVKTuu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YnXGKhy-I9Jy","executionInfo":{"status":"ok","timestamp":1733229597734,"user_tz":-420,"elapsed":11490,"user":{"displayName":"Nam Chan Nguyen","userId":"04673706717042935425"}},"outputId":"44e64331-64f7-481d-f7ec-2b01206051ad","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting streamlit\n","  Downloading streamlit-1.40.2-py2.py3-none-any.whl.metadata (8.4 kB)\n","Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n","Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n","Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n","Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n","Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n","Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.25.5)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n","Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n","Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n","Collecting watchdog<7,>=2.1.5 (from streamlit)\n","  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n","Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n","  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.21.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n","Downloading streamlit-1.40.2-py2.py3-none-any.whl (8.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n","Successfully installed pydeck-0.9.1 streamlit-1.40.2 watchdog-6.0.0\n","Collecting pyngrok\n","  Downloading pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n","Downloading pyngrok-7.2.1-py3-none-any.whl (22 kB)\n","Installing collected packages: pyngrok\n","Successfully installed pyngrok-7.2.1\n"]}],"source":["!pip3 install streamlit\n","!pip3 install pyngrok"]},{"cell_type":"code","source":["%%%writefile content_based_app.py\n","import streamlit as st\n","import pandas as pd\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from nltk.tokenize import word_tokenize\n","import pickle\n","\n","# Load your datasets\n","san_pham = pd.read_csv('San_pham.csv')\n","khach_hang = pd.read_csv('Khach_hang.csv')\n","danh_gia = pd.read_csv('Danh_gia.csv')\n","\n","# Load stopwords\n","with open('vietnamese-stopwords.txt', 'r', encoding=\"utf8\") as file:\n","    stopwords = file.read().split('\\n')\n","\n","\n","# Data preprocessing and merging\n","join_san_pham_danh_gia = pd.merge(danh_gia, san_pham, on='ma_san_pham', how='left')\n","df_ori = pd.merge(join_san_pham_danh_gia, khach_hang, on='ma_khach_hang', how='left')\n","dataframe = df_ori.copy()\n","\n","df = dataframe[['ma_san_pham', 'ten_san_pham', 'mo_ta', 'so_sao']]\n","\n","# Load replacement words\n","correct_word_list = pd.read_csv('correct_word_list.csv')\n","replacement_dict = dict(zip(correct_word_list['original'], correct_word_list['replacement']))\n","\n","# Function to clean and correct text\n","def clean_and_correct_text(text):\n","    # Clean text\n","    if isinstance(text, str):\n","        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n","        text = re.sub(r'\\d+', '', text)      # Remove numbers\n","        text = text.lower().strip()          # Lowercase and strip whitespace\n","        for key, value in replacement_dict.items():\n","            text = re.sub(r'\\b' + re.escape(key) + r'\\b', value, text)\n","        return text\n","    return ''\n","\n","df['mo_ta'] = df['mo_ta'].apply(clean_and_correct_text)\n","df['ten_san_pham'] = df['ten_san_pham'].apply(clean_and_correct_text)\n","\n","# Create combined content and tokenize\n","df['Content'] = df[['ten_san_pham', 'mo_ta']].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n","df[\"Content_wt\"] = df[\"Content\"].apply(lambda x: word_tokenize(x))\n","\n","# function cần thiết\n","\n","# Load cosine similarity matrix from the pickle file\n","with open('products_cosine_sim.pkl', 'rb') as f:\n","    cosine_sim = pickle.load(f)\n","\n","# Recommendation function\n","def get_recommendations_cosine(sp_id_or_name, cosine_sim=cosine_sim, nums=10, min_rating=4):\n","    if isinstance(sp_id_or_name, int):\n","        if sp_id_or_name not in df['ma_san_pham'].values:\n","            return pd.DataFrame()\n","        idx = df.index[df['ma_san_pham'] == sp_id_or_name][0]\n","    else:\n","        matching_products = df[df['ten_san_pham'].str.contains(sp_id_or_name, case=False, na=False) |\n","                               df['mo_ta'].str.contains(sp_id_or_name, case=False, na=False)]\n","        if matching_products.empty:\n","            return pd.DataFrame()\n","        idx = matching_products.index[0]\n","\n","    sim_scores = list(enumerate(cosine_sim[idx]))\n","    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n","    sim_scores = sim_scores[1:nums + 1]\n","\n","    sp_indices = [i[0] for i in sim_scores]\n","    recommended_products = df[['ma_san_pham', 'ten_san_pham', 'mo_ta', 'so_sao']].iloc[sp_indices]\n","    recommended_products = recommended_products[recommended_products['so_sao'] >= min_rating]\n","    recommended_products = recommended_products.drop_duplicates(subset='ma_san_pham').sort_values(by='so_sao', ascending=False)\n","\n","    return recommended_products\n","\n","# # Lấy 10 sản phẩm\n","# random_products = df.head(n=10)\n","\n","# # Hiển thị đề xuất ra bảng\n","# def display_recommended_products(recommended_products, cols=5):\n","#     for i in range(0, len(recommended_products), cols):\n","#         cols = st.columns(cols)\n","#         for j, col in enumerate(cols):\n","#             if i + j < len(recommended_products):\n","#                 product = recommended_products.iloc[i + j]\n","#                 with col:\n","#                     st.write(product['ten_san_pham'])\n","#                     expander = st.expander(f\"Mô tả\")\n","#                     product_description = product['mo_ta']\n","#                     truncated_description = ' '.join(product_description.split()[:100]) + '...'\n","#                     expander.write(truncated_description)\n","#                     expander.markdown(\"Nhấn vào mũi tên để đóng hộp text này.\")\n","\n","\n","# st.session_state.random_products = random_products\n","\n","st.title(\"Data Science Project\")\n","st.write(\"##\")\n","\n","menu = [\"Yêu cầu bài toán\", \"Xây dựng model\", \"Gợi ý cho người dùng\"]\n","choice = st.sidebar.selectbox('Menu', menu)\n","st.sidebar.write(\"\"\"#### Thành viên thực hiện:\n","                 1) CHẾ THỊ ÁNH TUYỀN\n","                 2) NGUYỄN CHẤN NAM\"\"\")\n","st.sidebar.write(\"\"\"#### Giảng viên hướng dẫn: \"\"\")\n","st.sidebar.write(\"\"\"#### Ngày báo cáo đồ án: 12/2024\"\"\")\n","if choice == 'Yêu cầu bài toán':\n","    st.subheader(\"Yêu cầu bài toán\")\n","    st.write(\"\"\"\n","    ###### Classifying spam and ham messages is one of the most common natural language processing tasks for emails and chat engines. With the advancements in machine learning and natural language processing techniques, it is now possible to separate spam messages from ham messages with a high degree of accuracy.\n","    \"\"\")\n","    st.write(\"\"\"###### => Problem/ Requirement: Use Machine Learning algorithms in Python for ham and spam message classification.\"\"\")\n","    # st.image(\"ham_spam.jpg\")\n","\n","elif choice == 'Xây dựng model':\n","    st.subheader(\"Xây dựng model\")\n","    st.write(\"##### 1. Some data\")\n","    # st.dataframe(data[['v2', 'v1']].head(3))\n","    # st.dataframe(data[['v2', 'v1']].tail(3))\n","    # st.write(\"##### 2. Visualize Ham and Spam\")\n","    # fig1 = sns.countplot(data=data[['v1']], x='v1')\n","    # st.pyplot(fig1.figure)\n","\n","    # st.write(\"##### 3. Xây dựng model...\")\n","    # st.write(\"##### 4. Evaluation\")\n","    # st.code(\"Score train:\"+ str(round(score_train,2)) + \" vs Score test:\" + str(round(score_test,2)))\n","    # st.code(\"Accuracy:\"+str(round(acc,2)))\n","    # st.write(\"###### Confusion matrix:\")\n","    # st.code(cm)\n","    # st.write(\"###### Classification report:\")\n","    # st.code(cr)\n","    # st.code(\"Roc AUC score:\" + str(round(roc,2)))\n","\n","    # # calculate roc curve\n","    # st.write(\"###### ROC curve\")\n","    # fpr, tpr, thresholds = roc_curve(y_test, y_prob[:, 1])\n","    # fig, ax = plt.subplots()\n","    # ax.plot([0, 1], [0, 1], linestyle='--')\n","    # ax.plot(fpr, tpr, marker='.')\n","    # st.pyplot(fig)\n","\n","    # st.write(\"##### 5. Summary: This model is good enough for Ham vs Spam classification.\")\n","\n","elif choice == 'Gợi ý cho người dùng':\n","    # Streamlit UI\n","    st.image('hasaki_banner.jpg')\n","    st.title('Product Recommendation System')\n","    # Display first 10 products\n","    st.subheader('Products List')\n","    st.dataframe(san_pham.head(10),)  # Display the first 10 products\n","\n","    user_input = st.text_input(\"Enter product ID or name:\")\n","    if st.button('Get Recommendations'):\n","        try:\n","            user_input_int = int(user_input)\n","            recommendations = get_recommendations_cosine(user_input_int)\n","        except ValueError:\n","            recommendations = get_recommendations_cosine(user_input)\n","            if not recommendations.empty:\n","                st.write(\"Recommendations:\")\n","                st.dataframe(recommendations)\n","            else:\n","                st.write(\"No recommendations available.\")\n","\n","\n","    # # Kiểm tra xem 'selected_ma_san_pham' đã có trong session_state hay chưa\n","    # if 'selected_ma_san_pham' not in st.session_state:\n","    #     # Nếu chưa có, thiết lập giá trị mặc định là None hoặc ID sản phẩm đầu tiên\n","    #     st.session_state.selected_ma_san_pham = None\n","\n","    # # Theo cách cho người dùng chọn sản phẩm từ dropdown\n","    # # Tạo một tuple cho mỗi sản phẩm, trong đó phần tử đầu là tên và phần tử thứ hai là ID\n","    # product_options = [(row['ten_san_pham'], row['ma_san_pham']) for index, row in st.session_state.random_products.iterrows()]\n","    # st.session_state.random_products\n","    # # Tạo một dropdown với options là các tuple này\n","    # selected_product = st.selectbox(\n","    #     \"Chọn sản phẩm\",\n","    #     options=product_options,\n","    #     format_func=lambda x: x[0]  # Hiển thị tên sản phẩm\n","    # )\n","    # # Display the selected product\n","    # st.write(\"Bạn đã chọn:\", selected_product)\n","\n","    # # Cập nhật session_state dựa trên lựa chọn hiện tại\n","    # st.session_state.selected_ma_san_pham = selected_product[1]\n","\n","    # if st.session_state.selected_ma_san_pham:\n","    #     st.write(\"ma_san_pham: \", st.session_state.selected_ma_san_pham)\n","    #     # Hiển thị thông tin sản phẩm được chọn\n","    #     selected_product = df[df['ma_san_pham'] == st.session_state.selected_ma_san_pham]\n","\n","    #     if not selected_product.empty:\n","    #         st.write('#### Bạn vừa chọn:')\n","    #         st.write('### ', selected_product['ten_san_pham'].values[0])\n","\n","    #         product_description = selected_product['mo_ta'].values[0]\n","    #         truncated_description = ' '.join(product_description.split()[:100])\n","    #         st.write('##### Thông tin:')\n","    #         st.write(truncated_description, '...')\n","\n","    #         st.write('##### Các sản phẩm liên quan:')\n","    #         recommendations = get_recommendations_cosine(df, st.session_state.selected_ma_san_pham, cosine_sim=cosine_sim_new, nums=3)\n","    #         display_recommended_products(recommendations, cols=3)\n","    #     else:\n","    #         st.write(f\"Không tìm thấy sản phẩm với ID: {st.session_state.selected_ma_san_pham}\")\n"],"metadata":{"id":"C4ymyC6WJH29","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"error","timestamp":1733229880524,"user_tz":-420,"elapsed":517,"user":{"displayName":"Nam Chan Nguyen","userId":"04673706717042935425"}},"outputId":"6d41395d-e17d-462d-f211-1d2d13239267"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["UsageError: Cell magic `%%%writefile` not found.\n"]}]},{"cell_type":"code","source":["from pyngrok import ngrok"],"metadata":{"id":"vUUst7jiKHL6","executionInfo":{"status":"ok","timestamp":1733229881543,"user_tz":-420,"elapsed":535,"user":{"displayName":"Nam Chan Nguyen","userId":"04673706717042935425"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["ngrok.set_auth_token(\"2pew2iJ1BvxuDjF1GQGsDExfbKo_4CimNeGDSWRK2PqR69hYb\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5w5fBipLKqe9","executionInfo":{"status":"ok","timestamp":1733229885612,"user_tz":-420,"elapsed":2533,"user":{"displayName":"Nam Chan Nguyen","userId":"04673706717042935425"}},"outputId":"3fd7e177-8895-4263-fd31-164b81ae4968"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":[]}]},{"cell_type":"code","source":["# Start Streamlit server on a specific port\n","! nohup streamlit run app.py --server.port 8501 &\n","# Start ngrok tunnel to expose the Streamlit server\n","ngrok_tunnel = ngrok.connect(addr='8501', proto='http', bind_tls=True)\n","# Print the URL of the ngrok tunnel\n","print(' * Tunnel URL:', ngrok tunnel.public url)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"OcebdvX2KD7D","executionInfo":{"status":"error","timestamp":1733229949043,"user_tz":-420,"elapsed":485,"user":{"displayName":"Nam Chan Nguyen","userId":"04673706717042935425"}},"outputId":"643de95b-d369-4504-a840-6f77af852de0"},"execution_count":6,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax. Perhaps you forgot a comma? (<ipython-input-6-95878e5e7047>, line 6)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-95878e5e7047>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    print(' * Tunnel URL:', ngrok tunnel.public url)\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"]}]},{"cell_type":"code","source":["# ngrok.kill()"],"metadata":{"id":"nTRWl6K-KLwj"},"execution_count":null,"outputs":[]}]}